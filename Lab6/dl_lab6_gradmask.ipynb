{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 274658,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 235179,
          "modelId": 256650
        }
      ],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def stochastic_relu(x, threshold=0.5):\n",
        "    return torch.where(x > threshold, x, torch.zeros_like(x))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-04T09:09:44.316910Z",
          "iopub.execute_input": "2025-03-04T09:09:44.317260Z",
          "iopub.status.idle": "2025-03-04T09:09:47.471249Z",
          "shell.execute_reply.started": "2025-03-04T09:09:44.317231Z",
          "shell.execute_reply": "2025-03-04T09:09:47.470323Z"
        },
        "id": "-KXi-Vw8dycC"
      },
      "outputs": [],
      "execution_count": 38
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size=3,\n",
        "            stride=stride,\n",
        "            padding=1,\n",
        "            bias=False,\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False\n",
        "        )\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(\n",
        "                    in_channels, out_channels, kernel_size=1, stride=stride, bias=False\n",
        "                ),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = stochastic_relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        return stochastic_relu(out)\n",
        "\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.layer1 = ResidualBlock(64, 128, stride=2)\n",
        "        self.layer2 = ResidualBlock(128, 256, stride=2)\n",
        "        self.layer3 = ResidualBlock(256, 512, stride=2)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = stochastic_relu(self.bn1(self.conv1(x)))\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.avg_pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dropout(x)\n",
        "        return self.fc(x)"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-04T09:09:48.236570Z",
          "iopub.execute_input": "2025-03-04T09:09:48.236999Z",
          "iopub.status.idle": "2025-03-04T09:09:48.246862Z",
          "shell.execute_reply.started": "2025-03-04T09:09:48.236971Z",
          "shell.execute_reply": "2025-03-04T09:09:48.246044Z"
        },
        "id": "p-kMTOLFdycI"
      },
      "outputs": [],
      "execution_count": 39
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform_train = transforms.Compose(\n",
        "    [\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "transform_test = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-04T03:33:06.034845Z",
          "iopub.execute_input": "2025-03-04T03:33:06.035128Z",
          "iopub.status.idle": "2025-03-04T03:33:08.422302Z",
          "shell.execute_reply.started": "2025-03-04T03:33:06.035099Z",
          "shell.execute_reply": "2025-03-04T03:33:08.421421Z"
        },
        "id": "FSJnARo9dycM"
      },
      "outputs": [],
      "execution_count": 40
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "train = torchvision.datasets.CIFAR10(root='./data',train=True,download=True,transform=transform_train)\n",
        "test = torchvision.datasets.CIFAR10(root='./data',train=False,download=False,transform=transform_test)\n",
        "trainloader = torch.utils.data.DataLoader(train,batch_size=128,shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(test,batch_size=128,shuffle=False)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-04T03:33:21.251874Z",
          "iopub.execute_input": "2025-03-04T03:33:21.252273Z",
          "iopub.status.idle": "2025-03-04T03:33:25.984327Z",
          "shell.execute_reply.started": "2025-03-04T03:33:21.252251Z",
          "shell.execute_reply": "2025-03-04T03:33:25.983429Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKwsJfhodycN",
        "outputId": "5d5b0101-15e1-4f90-96af-2b0038d23145"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "execution_count": 41
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-04T03:33:28.982102Z",
          "iopub.execute_input": "2025-03-04T03:33:28.982410Z",
          "iopub.status.idle": "2025-03-04T03:33:29.046702Z",
          "shell.execute_reply.started": "2025-03-04T03:33:28.982383Z",
          "shell.execute_reply": "2025-03-04T03:33:29.046088Z"
        },
        "id": "yerGEhrodycO"
      },
      "outputs": [],
      "execution_count": 42
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter()\n",
        "\n",
        "device = \"cuda\"\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
        "model = model.to(device)\n",
        "from tqdm import tqdm\n",
        "\n",
        "lr = 0.001\n",
        "for i in range(20):\n",
        "    correct, total, running_loss = 0, 0, 0\n",
        "    train_bar = tqdm(trainloader, desc=f'Train Epoch {i}')\n",
        "\n",
        "    for image, label in train_bar:\n",
        "        image, label = image.to(device), label.to(device)\n",
        "\n",
        "        out = model(image)\n",
        "        loss_value = loss(out, label)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss_value.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        predictions = out.argmax(dim=1)\n",
        "        correct += (predictions == label).sum().item()\n",
        "        total += label.size(0)\n",
        "        running_loss += loss_value.item()\n",
        "\n",
        "        train_bar.set_postfix(loss=running_loss / total, acc=100 * correct / total)\n",
        "        writer.add_scalar('Loss/train', running_loss / total, i)\n",
        "        writer.add_scalar('Accuracy/train', 100 * correct / total, i)\n",
        "    correct, total, test_loss = 0, 0, 0\n",
        "    test_bar = tqdm(testloader, desc=f'Test Epoch {i}')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for test, test_label in test_bar:\n",
        "            test, test_label = test.to(device), test_label.to(device)\n",
        "\n",
        "            test_out = model(test)\n",
        "            loss_value = loss(test_out, test_label)\n",
        "            test_loss += loss_value.item()\n",
        "\n",
        "            predictions = test_out.argmax(dim=1)\n",
        "            correct += (predictions == test_label).sum().item()\n",
        "            total += test_label.size(0)\n",
        "\n",
        "            test_bar.set_postfix(loss=test_loss / total, acc=100 * correct / total)\n",
        "            writer.add_scalar('Loss/test', test_loss / total, i)\n",
        "            writer.add_scalar('Accuracy/test', 100 * correct / total, i)\n",
        "    scheduler.step(test_loss)\n",
        "    if optimizer.param_groups[0][\"lr\"] != lr:\n",
        "        lr = optimizer.param_groups[0][\"lr\"]\n",
        "        print(f\"Updated learning rate: {lr}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-04T03:33:30.251079Z",
          "iopub.execute_input": "2025-03-04T03:33:30.251357Z",
          "execution_failed": "2025-03-04T03:54:11.941Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XX4UY6RDdycO",
        "outputId": "7b688fd0-2b47-4c61-adbe-949227caaea0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 0: 100%|██████████| 391/391 [00:36<00:00, 10.68it/s, acc=50.4, loss=0.0106]\n",
            "Test Epoch 0: 100%|██████████| 79/79 [00:03<00:00, 22.24it/s, acc=59.1, loss=0.00906]\n",
            "Train Epoch 1: 100%|██████████| 391/391 [00:34<00:00, 11.33it/s, acc=66.9, loss=0.00732]\n",
            "Test Epoch 1: 100%|██████████| 79/79 [00:03<00:00, 24.08it/s, acc=69.7, loss=0.00693]\n",
            "Train Epoch 2: 100%|██████████| 391/391 [00:34<00:00, 11.32it/s, acc=74, loss=0.00585]\n",
            "Test Epoch 2: 100%|██████████| 79/79 [00:03<00:00, 24.36it/s, acc=74.5, loss=0.00594]\n",
            "Train Epoch 3: 100%|██████████| 391/391 [00:34<00:00, 11.38it/s, acc=77.6, loss=0.00506]\n",
            "Test Epoch 3: 100%|██████████| 79/79 [00:03<00:00, 24.11it/s, acc=76.2, loss=0.00547]\n",
            "Train Epoch 4: 100%|██████████| 391/391 [00:34<00:00, 11.35it/s, acc=79.5, loss=0.00461]\n",
            "Test Epoch 4: 100%|██████████| 79/79 [00:03<00:00, 24.09it/s, acc=78.8, loss=0.00493]\n",
            "Train Epoch 5: 100%|██████████| 391/391 [00:34<00:00, 11.40it/s, acc=81, loss=0.0043]\n",
            "Test Epoch 5: 100%|██████████| 79/79 [00:03<00:00, 23.82it/s, acc=78.6, loss=0.00501]\n",
            "Train Epoch 6: 100%|██████████| 391/391 [00:34<00:00, 11.33it/s, acc=82.2, loss=0.00407]\n",
            "Test Epoch 6: 100%|██████████| 79/79 [00:03<00:00, 21.01it/s, acc=78, loss=0.00507]\n",
            "Train Epoch 7: 100%|██████████| 391/391 [00:34<00:00, 11.44it/s, acc=83.1, loss=0.00383]\n",
            "Test Epoch 7: 100%|██████████| 79/79 [00:03<00:00, 20.00it/s, acc=78.5, loss=0.00502]\n",
            "Train Epoch 8: 100%|██████████| 391/391 [00:34<00:00, 11.38it/s, acc=83.8, loss=0.00371]\n",
            "Test Epoch 8: 100%|██████████| 79/79 [00:03<00:00, 23.50it/s, acc=80.1, loss=0.0046]\n",
            "Train Epoch 9: 100%|██████████| 391/391 [00:34<00:00, 11.27it/s, acc=84.2, loss=0.00363]\n",
            "Test Epoch 9: 100%|██████████| 79/79 [00:03<00:00, 24.67it/s, acc=80.2, loss=0.00458]\n",
            "Train Epoch 10: 100%|██████████| 391/391 [00:34<00:00, 11.30it/s, acc=84.8, loss=0.00348]\n",
            "Test Epoch 10: 100%|██████████| 79/79 [00:03<00:00, 24.65it/s, acc=81.4, loss=0.00442]\n",
            "Train Epoch 11: 100%|██████████| 391/391 [00:34<00:00, 11.36it/s, acc=84.9, loss=0.00346]\n",
            "Test Epoch 11: 100%|██████████| 79/79 [00:03<00:00, 23.70it/s, acc=80, loss=0.00462]\n",
            "Train Epoch 12: 100%|██████████| 391/391 [00:34<00:00, 11.33it/s, acc=85.1, loss=0.00339]\n",
            "Test Epoch 12: 100%|██████████| 79/79 [00:03<00:00, 23.75it/s, acc=81.3, loss=0.00446]\n",
            "Train Epoch 13: 100%|██████████| 391/391 [00:34<00:00, 11.26it/s, acc=85.7, loss=0.00328]\n",
            "Test Epoch 13: 100%|██████████| 79/79 [00:03<00:00, 20.43it/s, acc=80.2, loss=0.00463]\n",
            "Train Epoch 14: 100%|██████████| 391/391 [00:35<00:00, 11.08it/s, acc=85.6, loss=0.0033]\n",
            "Test Epoch 14: 100%|██████████| 79/79 [00:03<00:00, 21.87it/s, acc=80.8, loss=0.00454]\n",
            "Train Epoch 15: 100%|██████████| 391/391 [00:35<00:00, 11.14it/s, acc=85.7, loss=0.00324]\n",
            "Test Epoch 15: 100%|██████████| 79/79 [00:03<00:00, 23.37it/s, acc=81.8, loss=0.00428]\n",
            "Train Epoch 16: 100%|██████████| 391/391 [00:34<00:00, 11.23it/s, acc=86, loss=0.00322]\n",
            "Test Epoch 16: 100%|██████████| 79/79 [00:03<00:00, 23.98it/s, acc=81.9, loss=0.00425]\n",
            "Train Epoch 17: 100%|██████████| 391/391 [00:35<00:00, 11.12it/s, acc=86.1, loss=0.00321]\n",
            "Test Epoch 17: 100%|██████████| 79/79 [00:03<00:00, 21.80it/s, acc=82.7, loss=0.0041]\n",
            "Train Epoch 18: 100%|██████████| 391/391 [00:34<00:00, 11.33it/s, acc=85.7, loss=0.00324]\n",
            "Test Epoch 18: 100%|██████████| 79/79 [00:03<00:00, 20.30it/s, acc=82.2, loss=0.00418]\n",
            "Train Epoch 19: 100%|██████████| 391/391 [00:34<00:00, 11.31it/s, acc=85.6, loss=0.00325]\n",
            "Test Epoch 19: 100%|██████████| 79/79 [00:03<00:00, 23.67it/s, acc=82.4, loss=0.00413]\n"
          ]
        }
      ],
      "execution_count": 43
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'cifar10_cnn_grad_masking.pth')"
      ],
      "metadata": {
        "trusted": true,
        "id": "Rjd8fEjAdycQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "target_class = torch.tensor([9], dtype=torch.long, device=\"cuda\")\n",
        "model = model.to(\"cuda\")\n",
        "model.eval()\n",
        "c = 0\n",
        "l = 0\n",
        "for i in range(100):\n",
        "    j = random.randint(1, len(train))\n",
        "    input_image = train[j][0].unsqueeze(0).to(\"cuda\")\n",
        "    input_image = input_image.detach().clone()\n",
        "    input_image.requires_grad_(True)\n",
        "    output = model(input_image)\n",
        "    loss = -torch.nn.functional.cross_entropy(output, target_class)\n",
        "\n",
        "    model.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        input_image += 0.1 * input_image.grad.sign()\n",
        "        input_image.clamp_(0, 1)\n",
        "    input_image.grad.zero_()\n",
        "    a = model(input_image)\n",
        "    pred = torch.argmax(torch.nn.functional.softmax(a, dim=1)).item()\n",
        "    if pred == train[j][1]:\n",
        "        c += 1\n",
        "    l += loss.item()\n",
        "print(c / 100)\n",
        "print(\"average loss\", l / 100)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-04T09:16:19.333174Z",
          "iopub.execute_input": "2025-03-04T09:16:19.333475Z",
          "iopub.status.idle": "2025-03-04T09:16:19.373322Z",
          "shell.execute_reply.started": "2025-03-04T09:16:19.333455Z",
          "shell.execute_reply": "2025-03-04T09:16:19.372479Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcx_Ogv9dycU",
        "outputId": "408dae79-1295-4513-85d4-882930129d76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.47\n",
            "average loss -6.746211655332008\n"
          ]
        }
      ],
      "execution_count": 56
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "Y8ccLWZTdycW"
      },
      "outputs": [],
      "execution_count": 49
    }
  ]
}