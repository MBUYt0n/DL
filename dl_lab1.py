# -*- coding: utf-8 -*-
"""DL_Lab1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HHqGoAgmgcKt7SGeV-RF6Ujmy_CoB2by
"""

import torch
import torchvision

# train = torchvision.datasets.MNIST(root="./", train=True, download=True)
# test = torchvision.datasets.MNIST(root="./", train=False, download=True)

import tensorflow
(x_train, y_train), (x_test, y_test) = tensorflow.keras.datasets.mnist.load_data()
x_train = torch.tensor(x_train, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.uint8)
x_test = torch.tensor(x_test, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.uint8)

x_train[0].size()

# x_train = x_train / 255
# x_valid = x_valid / 255

y_train.dtype

trainloader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_train, y_train), batch_size=20, shuffle=True)
validloader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_test, y_test), batch_size=20, shuffle=True)

for i, j in enumerate(trainloader):
  print(i)
  print(j[0][0].size())
  break

import torch.nn as nn

model = nn.Sequential(
    nn.Linear(784, 100),
    nn.Sigmoid(),
    nn.Linear(100, 100),
    nn.Sigmoid(),
    nn.Linear(100, 100),
    nn.Sigmoid(),
    nn.Linear(100, 10)
).to("cuda")

loss = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.003)

# from torch.utils.tensorboard import SummaryWriter
# writer = SummaryWriter()

from tqdm import tqdm

for i in range(100):
    model.train()
    train_progress = tqdm(
        enumerate(trainloader), desc=f"Epoch {i} - Training", total=len(trainloader)
    )
    for j, k in train_progress:
        image, label = k
        image, label = image.to("cuda"), label.to("cuda")
        optimizer.zero_grad()
        output = model(image.view(-1, 784))
        l = loss(output, label)
        l.backward()
        optimizer.step()
        # Update tqdm with current loss
        train_progress.set_postfix(loss=l.item())
    # writer.add_scalar("Loss/train", t, i)

    model.eval()
    val_progress = tqdm(
        enumerate(validloader), desc=f"Epoch {i} - Validation", total=len(validloader)
    )
    for j, k in val_progress:
        image, label = k
        image, label = image.to("cuda"), label.to("cuda")
        output = model(image.view(-1, 784))
        l = loss(output, label)
        # Update tqdm with current validation loss
        val_progress.set_postfix(loss=l.item())
    # writer.add_scalar("Loss/val", t, i)
    # writer.flush()


# # Commented out IPython magic to ensure Python compatibility.
# # %load_ext tensorboard
# # %tensorboard --logdir=runs

# t = 0
# for j, k in zip(x_valid, y_valid):
#     image = j.to("cuda")
#     with torch.no_grad():
#       output = model(image.view(-1, 784))
#     output = torch.nn.Softmax(dim=1)(output)
#     output = torch.argmax(output).to("cpu")
#     if output == k:
#       t += 1


# print(t/len(y_valid))

# import torch
# import numpy as np
# from sklearn.metrics import confusion_matrix
# import matplotlib.pyplot as plt
# import seaborn as sns

# num_classes = 10
# all_preds = []
# all_labels = []

# for images, labels in validloader:
#     images, labels = images.to("cuda"), labels.to("cuda")

#     with torch.no_grad():
#         outputs = model(images.view(-1, 784))
#         outputs = torch.nn.Softmax(dim=1)(outputs)
#         preds = torch.argmax(outputs, dim=1)

#     all_preds.extend(preds.cpu().numpy())
#     all_labels.extend(labels.cpu().numpy())

# conf_matrix = confusion_matrix(all_labels, all_preds, labels=np.arange(num_classes))

# plt.figure(figsize=(10, 8))
# sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.arange(num_classes),
#             yticklabels=np.arange(num_classes))
# plt.xlabel('Predicted Labels')
# plt.ylabel('True Labels')
# plt.title('Confusion Matrix')
# plt.show()

# def model_maker(layers, neurons):
#     model = nn.Sequential()
#     for i in range(layers):
#         if i == 0:
#             model.add(nn.Linear(784, neurons))
#         else:
#           model.add(nn.Linear(neurons, neurons))
#         model.add(nn.Sigmoid())
#     model.add(nn.Linear(neurons, 10))
#     return model

# def train_loop(model, optimizer, loss, run_name):
#     writer = SummaryWriter(log_dir=f"runs/{run_name}")
#     for i in range(100):
#         t = 0
#         model.train()
#         for j, k in enumerate(trainloader):
#             image, label = k
#             image, label = image.to("cuda"), label.to("cuda")
#             optimizer.zero_grad()
#             output = model(image.view(-1, 784))
#             l = loss(output, label)
#             t += l.item()
#             l.backward()
#             optimizer.step()
#         t /= len(trainloader)
#         writer.add_scalar("Loss/train", t, i)
#         print(f"Epoch {i}, Loss {t}")

#         t = 0
#         model.eval()
#         for j, k in enumerate(validloader):
#             image, label = k
#             image, label = image.to("cuda"), label.to("cuda")
#             output = model(image.view(-1, 784))
#             l = loss(output, label)
#             t += l.item()
#         t /= len(validloader)
#         print(f"Val Loss {t}")
#         writer.add_scalar("Loss/val", t, i)
#         writer.flush()
#     writer.close()
