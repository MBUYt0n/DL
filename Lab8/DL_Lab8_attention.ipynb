{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91ypfmbwAU3V",
        "outputId": "c0915752-ee67-48a3-996d-c6b66686449b"
      },
      "outputs": [],
      "source": [
        "# import kagglehub\n",
        "\n",
        "# # Download latest version\n",
        "# path = kagglehub.dataset_download(\"shusrith/machine-trainslation\")\n",
        "\n",
        "# print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "ZjgPhK4yAU3W",
        "outputId": "4e13acb7-bfda-44e6-e138-db208b581419"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>Spanish</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>114623</th>\n",
              "      <td>[2, 22056, 14224, 449, 7920, 674, 930, 13054, ...</td>\n",
              "      <td>[2, 664, 22056, 14225, 17303, 294, 1850, 2761,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57225</th>\n",
              "      <td>[2, 270, 3279, 673, 3968, 7959, 224, 3, 0, 0, ...</td>\n",
              "      <td>[2, 566, 294, 1986, 355, 4578, 387, 8624, 224,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35491</th>\n",
              "      <td>[2, 270, 384, 684, 272, 341, 370, 224, 3, 0, 0...</td>\n",
              "      <td>[2, 535, 2668, 265, 550, 563, 224, 3, 0, 0, 0,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78889</th>\n",
              "      <td>[2, 270, 431, 574, 270, 411, 496, 3540, 224, 3...</td>\n",
              "      <td>[2, 302, 850, 316, 4208, 7143, 224, 3, 0, 0, 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102889</th>\n",
              "      <td>[2, 270, 807, 3159, 272, 718, 328, 3452, 300, ...</td>\n",
              "      <td>[2, 372, 3576, 265, 1234, 316, 328, 6715, 265,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83704</th>\n",
              "      <td>[2, 826, 270, 1364, 391, 448, 7489, 2209, 224,...</td>\n",
              "      <td>[2, 665, 2053, 556, 1135, 322, 474, 5449, 224,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106144</th>\n",
              "      <td>[2, 290, 1578, 462, 4486, 24099, 2349, 300, 21...</td>\n",
              "      <td>[2, 290, 3346, 801, 4929, 294, 5116, 1941, 294...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58936</th>\n",
              "      <td>[2, 442, 1354, 521, 1083, 1251, 1847, 224, 3, ...</td>\n",
              "      <td>[2, 463, 372, 7591, 986, 1093, 1081, 224, 3, 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79731</th>\n",
              "      <td>[2, 361, 384, 402, 1344, 272, 2320, 300, 1024,...</td>\n",
              "      <td>[2, 767, 403, 7029, 470, 4485, 321, 1432, 224,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12882</th>\n",
              "      <td>[2, 370, 1620, 29405, 224, 3, 0, 0, 0, 0, 0, 0...</td>\n",
              "      <td>[2, 563, 336, 537, 1253, 4027, 224, 3, 0, 0, 0...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>128982 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  English  \\\n",
              "114623  [2, 22056, 14224, 449, 7920, 674, 930, 13054, ...   \n",
              "57225   [2, 270, 3279, 673, 3968, 7959, 224, 3, 0, 0, ...   \n",
              "35491   [2, 270, 384, 684, 272, 341, 370, 224, 3, 0, 0...   \n",
              "78889   [2, 270, 431, 574, 270, 411, 496, 3540, 224, 3...   \n",
              "102889  [2, 270, 807, 3159, 272, 718, 328, 3452, 300, ...   \n",
              "...                                                   ...   \n",
              "83704   [2, 826, 270, 1364, 391, 448, 7489, 2209, 224,...   \n",
              "106144  [2, 290, 1578, 462, 4486, 24099, 2349, 300, 21...   \n",
              "58936   [2, 442, 1354, 521, 1083, 1251, 1847, 224, 3, ...   \n",
              "79731   [2, 361, 384, 402, 1344, 272, 2320, 300, 1024,...   \n",
              "12882   [2, 370, 1620, 29405, 224, 3, 0, 0, 0, 0, 0, 0...   \n",
              "\n",
              "                                                  Spanish  \n",
              "114623  [2, 664, 22056, 14225, 17303, 294, 1850, 2761,...  \n",
              "57225   [2, 566, 294, 1986, 355, 4578, 387, 8624, 224,...  \n",
              "35491   [2, 535, 2668, 265, 550, 563, 224, 3, 0, 0, 0,...  \n",
              "78889   [2, 302, 850, 316, 4208, 7143, 224, 3, 0, 0, 0...  \n",
              "102889  [2, 372, 3576, 265, 1234, 316, 328, 6715, 265,...  \n",
              "...                                                   ...  \n",
              "83704   [2, 665, 2053, 556, 1135, 322, 474, 5449, 224,...  \n",
              "106144  [2, 290, 3346, 801, 4929, 294, 5116, 1941, 294...  \n",
              "58936   [2, 463, 372, 7591, 986, 1093, 1081, 224, 3, 0...  \n",
              "79731   [2, 767, 403, 7029, 470, 4485, 321, 1432, 224,...  \n",
              "12882   [2, 563, 336, 537, 1253, 4027, 224, 3, 0, 0, 0...  \n",
              "\n",
              "[128982 rows x 2 columns]"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "df = pd.read_csv(f\"EnglishOrSpanish/output_joint1.csv\")\n",
        "df = shuffle(df)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "3l1WLpiLAU3W",
        "outputId": "0b120dd3-332c-4483-dd20-5e629441a780"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>Spanish</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>114623</th>\n",
              "      <td>[2, 22056, 14224, 449, 7920, 674, 930, 13054, ...</td>\n",
              "      <td>[2, 664, 22056, 14225, 17303, 294, 1850, 2761,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57225</th>\n",
              "      <td>[2, 270, 3279, 673, 3968, 7959, 224, 3, 0, 0, ...</td>\n",
              "      <td>[2, 566, 294, 1986, 355, 4578, 387, 8624, 224,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35491</th>\n",
              "      <td>[2, 270, 384, 684, 272, 341, 370, 224, 3, 0, 0...</td>\n",
              "      <td>[2, 535, 2668, 265, 550, 563, 224, 3, 0, 0, 0,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78889</th>\n",
              "      <td>[2, 270, 431, 574, 270, 411, 496, 3540, 224, 3...</td>\n",
              "      <td>[2, 302, 850, 316, 4208, 7143, 224, 3, 0, 0, 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102889</th>\n",
              "      <td>[2, 270, 807, 3159, 272, 718, 328, 3452, 300, ...</td>\n",
              "      <td>[2, 372, 3576, 265, 1234, 316, 328, 6715, 265,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83704</th>\n",
              "      <td>[2, 826, 270, 1364, 391, 448, 7489, 2209, 224,...</td>\n",
              "      <td>[2, 665, 2053, 556, 1135, 322, 474, 5449, 224,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106144</th>\n",
              "      <td>[2, 290, 1578, 462, 4486, 24099, 2349, 300, 21...</td>\n",
              "      <td>[2, 290, 3346, 801, 4929, 294, 5116, 1941, 294...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58936</th>\n",
              "      <td>[2, 442, 1354, 521, 1083, 1251, 1847, 224, 3, ...</td>\n",
              "      <td>[2, 463, 372, 7591, 986, 1093, 1081, 224, 3, 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79731</th>\n",
              "      <td>[2, 361, 384, 402, 1344, 272, 2320, 300, 1024,...</td>\n",
              "      <td>[2, 767, 403, 7029, 470, 4485, 321, 1432, 224,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12882</th>\n",
              "      <td>[2, 370, 1620, 29405, 224, 3, 0, 0, 0, 0, 0, 0...</td>\n",
              "      <td>[2, 563, 336, 537, 1253, 4027, 224, 3, 0, 0, 0...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>128982 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  English  \\\n",
              "114623  [2, 22056, 14224, 449, 7920, 674, 930, 13054, ...   \n",
              "57225   [2, 270, 3279, 673, 3968, 7959, 224, 3, 0, 0, ...   \n",
              "35491   [2, 270, 384, 684, 272, 341, 370, 224, 3, 0, 0...   \n",
              "78889   [2, 270, 431, 574, 270, 411, 496, 3540, 224, 3...   \n",
              "102889  [2, 270, 807, 3159, 272, 718, 328, 3452, 300, ...   \n",
              "...                                                   ...   \n",
              "83704   [2, 826, 270, 1364, 391, 448, 7489, 2209, 224,...   \n",
              "106144  [2, 290, 1578, 462, 4486, 24099, 2349, 300, 21...   \n",
              "58936   [2, 442, 1354, 521, 1083, 1251, 1847, 224, 3, ...   \n",
              "79731   [2, 361, 384, 402, 1344, 272, 2320, 300, 1024,...   \n",
              "12882   [2, 370, 1620, 29405, 224, 3, 0, 0, 0, 0, 0, 0...   \n",
              "\n",
              "                                                  Spanish  \n",
              "114623  [2, 664, 22056, 14225, 17303, 294, 1850, 2761,...  \n",
              "57225   [2, 566, 294, 1986, 355, 4578, 387, 8624, 224,...  \n",
              "35491   [2, 535, 2668, 265, 550, 563, 224, 3, 0, 0, 0,...  \n",
              "78889   [2, 302, 850, 316, 4208, 7143, 224, 3, 0, 0, 0...  \n",
              "102889  [2, 372, 3576, 265, 1234, 316, 328, 6715, 265,...  \n",
              "...                                                   ...  \n",
              "83704   [2, 665, 2053, 556, 1135, 322, 474, 5449, 224,...  \n",
              "106144  [2, 290, 3346, 801, 4929, 294, 5116, 1941, 294...  \n",
              "58936   [2, 463, 372, 7591, 986, 1093, 1081, 224, 3, 0...  \n",
              "79731   [2, 767, 403, 7029, 470, 4485, 321, 1432, 224,...  \n",
              "12882   [2, 563, 336, 537, 1253, 4027, 224, 3, 0, 0, 0...  \n",
              "\n",
              "[128982 rows x 2 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import ast\n",
        "\n",
        "df[\"English\"] = df[\"English\"].apply(ast.literal_eval)\n",
        "df[\"Spanish\"] = df[\"Spanish\"].apply(ast.literal_eval)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "p8fPYQBWAU3X"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "train = TensorDataset(\n",
        "    torch.tensor(df[\"English\"][:100000].tolist(), dtype=torch.long),\n",
        "    torch.tensor(df[\"Spanish\"][:100000].tolist(), dtype=torch.long),\n",
        ")\n",
        "test = TensorDataset(\n",
        "    torch.tensor(df[\"English\"][100000:].tolist(), dtype=torch.long),\n",
        "    torch.tensor(df[\"Spanish\"][100000:].tolist(), dtype=torch.long),\n",
        ")\n",
        "train_loader = DataLoader(train, batch_size=256, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(test, batch_size=256, shuffle=False, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_9tn5YdUAU3X"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open(f\"EnglishOrSpanish/vocab.json\", \"r\") as f:\n",
        "    vocab = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KIMlq-cdAU3Y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers=1, dropout=0.3):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(\n",
        "            hidden_size,\n",
        "            hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "\n",
        "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
        "        self.hidden_norm = nn.LayerNorm(hidden_size)\n",
        "        self.cell_norm = nn.LayerNorm(hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.LSTM):\n",
        "            for name, param in module.named_parameters():\n",
        "                if \"weight\" in name:\n",
        "                    nn.init.xavier_uniform_(param)\n",
        "                elif \"bias\" in name:\n",
        "                    nn.init.zeros_(param)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.dropout(self.embedding(x))\n",
        "        output, (hidden, cell) = self.lstm(embedded)\n",
        "        output = self.layer_norm(output)\n",
        "        hidden = self.hidden_norm(hidden)\n",
        "        cell = self.cell_norm(cell)\n",
        "        return output, hidden, cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7MEXFtjhAU3Y"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Attention, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.attn = nn.Linear(hidden_size, hidden_size)\n",
        "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
        "\n",
        "    def forward(self, hidden, encoder_output):\n",
        "        hidden = hidden.unsqueeze(1)\n",
        "        scores = self.attn(encoder_output)\n",
        "        scores = torch.tanh(scores + hidden)\n",
        "        attention = self.v(scores).squeeze(2)\n",
        "        return nn.functional.softmax(attention, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "B-kd_lvxAU3Y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, attention, num_layers=1, dropout=0.3):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size, padding_idx=0)\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            2 * hidden_size,\n",
        "            hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "        self.attention = attention\n",
        "\n",
        "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
        "        self.hidden_norm = nn.LayerNorm(hidden_size)\n",
        "        self.cell_norm = nn.LayerNorm(hidden_size)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.LSTM):\n",
        "            for name, param in module.named_parameters():\n",
        "                if \"weight\" in name:\n",
        "                    nn.init.xavier_uniform_(param)\n",
        "                elif \"bias\" in name:\n",
        "                    nn.init.zeros_(param)\n",
        "\n",
        "    def forward(self, x, hidden, cell, encoder_output):\n",
        "        attention_scores = self.attention(hidden[-1], encoder_output)\n",
        "        context = torch.bmm(attention_scores.unsqueeze(1), encoder_output)\n",
        "        embedded = self.dropout(self.embedding(x)).unsqueeze(1)\n",
        "        embedded = torch.cat([embedded, context], dim=2)\n",
        "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "        output = self.layer_norm(output)\n",
        "        hidden = self.hidden_norm(hidden)\n",
        "        cell = self.cell_norm(cell)\n",
        "        output = self.fc(output)\n",
        "        return output, hidden, cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "oZnO_wztAU3Y"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq:\n",
        "    def __init__(self, encoder, decoder, device, teacher_forcing_ratio):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        self.vocab_size = self.decoder.vocab_size\n",
        "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        train_loader,\n",
        "        test_loader,\n",
        "        enc_optimizer,\n",
        "        dec_optimizer,\n",
        "        criterion,\n",
        "        device,\n",
        "        encoder_scheduler,\n",
        "        decoder_scheduler,\n",
        "        num_epochs,\n",
        "    ):\n",
        "        prev_lr_enc = enc_optimizer.param_groups[0][\"lr\"]\n",
        "        prev_lr_dec = dec_optimizer.param_groups[0][\"lr\"]\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "            self.encoder.train()\n",
        "            self.decoder.train()\n",
        "            epoch_loss = 0\n",
        "            epoch_acc = 0\n",
        "            progress_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
        "\n",
        "            for src, trg in progress_bar:\n",
        "                src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "                batch_size, trg_len = trg.shape\n",
        "\n",
        "                enc_optimizer.zero_grad()\n",
        "                dec_optimizer.zero_grad()\n",
        "\n",
        "                encoder_out, hidden, cell = self.encoder(src)\n",
        "                outputs = torch.zeros(batch_size, trg_len, self.vocab_size).to(\n",
        "                    self.device\n",
        "                )\n",
        "                x = trg[:, 0]\n",
        "                for t in range(1, trg_len):\n",
        "                    output, hidden, cell = self.decoder(x, hidden, cell, encoder_out)\n",
        "                    output = output.squeeze(1)\n",
        "                    outputs[:, t, :] = output\n",
        "                    teacher_force = torch.rand(1).item() < self.teacher_forcing_ratio\n",
        "                    x = trg[:, t] if teacher_force else output.argmax(dim=1)\n",
        "                loss = criterion(\n",
        "                    outputs[:, 1:].reshape(-1, self.vocab_size), trg[:, 1:].reshape(-1)\n",
        "                )\n",
        "                loss.backward()\n",
        "                enc_optimizer.step()\n",
        "                dec_optimizer.step()\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                preds = outputs.argmax(dim=2)\n",
        "                correct = (preds == trg).float().sum().item()\n",
        "                total = trg.numel()\n",
        "                batch_acc = correct / total\n",
        "                epoch_acc += batch_acc\n",
        "\n",
        "                progress_bar.set_postfix(\n",
        "                    loss=f\"{loss.item():.4f}\", acc=f\"{batch_acc:.4f}\"\n",
        "                )\n",
        "\n",
        "            train_loss = epoch_loss / len(train_loader)\n",
        "            train_acc = epoch_acc / len(train_loader)\n",
        "\n",
        "            # ---------------- VALIDATION ---------------- #\n",
        "            self.encoder.eval()\n",
        "            self.decoder.eval()\n",
        "            val_epoch_loss = 0\n",
        "            val_epoch_acc = 0\n",
        "            progress_bar = tqdm(test_loader, desc=\"Validating\", leave=False)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for src, trg in progress_bar:\n",
        "                    src, trg = src.to(device), trg.to(device)\n",
        "                    batch_size, trg_len = trg.shape\n",
        "                    encoder_out, hidden, cell = self.encoder(src)\n",
        "                    outputs = torch.zeros(batch_size, trg_len, self.vocab_size).to(\n",
        "                        self.device\n",
        "                    )\n",
        "                    x = trg[:, 0]\n",
        "                    for t in range(1, trg_len):\n",
        "                        output, hidden, cell = self.decoder(\n",
        "                            x, hidden, cell, encoder_out\n",
        "                        )\n",
        "                        output = output.squeeze(1)\n",
        "                        outputs[:, t, :] = output\n",
        "                        x = output.argmax(dim=1)\n",
        "\n",
        "                    loss = criterion(\n",
        "                        outputs[:, 1:].reshape(-1, self.vocab_size),\n",
        "                        trg[:, 1:].reshape(-1),\n",
        "                    )\n",
        "                    val_epoch_loss += loss.item()\n",
        "\n",
        "                    preds = outputs.argmax(dim=2)\n",
        "                    correct = (preds == trg).float().sum().item()\n",
        "                    total = trg.numel()\n",
        "                    batch_acc = correct / total\n",
        "                    val_epoch_acc += batch_acc\n",
        "\n",
        "                    progress_bar.set_postfix(\n",
        "                        loss=f\"{loss.item():.4f}\", acc=f\"{batch_acc:.4f}\"\n",
        "                    )\n",
        "\n",
        "            val_loss = val_epoch_loss / len(test_loader)\n",
        "            val_acc = val_epoch_acc / len(test_loader)\n",
        "            print(\n",
        "                f\"Train loss : {train_loss}, Train accuracy : {train_acc}, Val_loss : {val_loss}, val accuracy : {val_acc}\"\n",
        "            )\n",
        "            encoder_scheduler.step(val_loss)\n",
        "            decoder_scheduler.step(val_loss)\n",
        "            if encoder_scheduler.get_last_lr()[0] < prev_lr_enc:\n",
        "                prev_lr_enc = encoder_scheduler.get_last_lr()[0]\n",
        "                print(f\"Encoder learning rate decreased to {prev_lr_enc}\")\n",
        "            if decoder_scheduler.get_last_lr()[0] < prev_lr_dec:\n",
        "                prev_lr_dec = decoder_scheduler.get_last_lr()[0]\n",
        "        return train_loss, train_acc, val_loss, val_acc\n",
        "\n",
        "    def predict(self, src, trg):\n",
        "        self.encoder.eval()\n",
        "        self.decoder.eval()\n",
        "        with torch.no_grad():\n",
        "            src, trg = src.to(self.device), trg.to(self.device)\n",
        "            trg_len = trg.shape[0]\n",
        "            _, hidden, cell = self.encoder(src)\n",
        "            outputs = torch.zeros(1, trg_len, self.vocab_size).to(self.device)\n",
        "            x = trg[0]\n",
        "            for t in range(1, trg_len):\n",
        "                output, hidden, cell = self.decoder(x, hidden, cell)\n",
        "                outputs[:, t, :] = output\n",
        "                x = output.argmax(dim=1)\n",
        "            return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rXj3XkiAAU3a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/shusrith/projects/torch/lib/python3.12/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "hidden_size = 256\n",
        "encoder = Encoder(vocab_size, hidden_size).to(device)\n",
        "attention = Attention(hidden_size)\n",
        "decoder = Decoder(vocab_size, hidden_size, attention).to(device)\n",
        "seq2seq = Seq2Seq(encoder, decoder, device, 0.5)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.005, weight_decay=1e-5)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.005, weight_decay=1e-5)\n",
        "encoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    encoder_optimizer, mode=\"min\", factor=0.6, patience=2, min_lr=1e-6\n",
        ")\n",
        "decoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    decoder_optimizer, mode=\"min\", factor=0.6, patience=2, min_lr=1e-6\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsk2HEv5AU3a",
        "outputId": "287bd0ca-129c-4dcd-f6ef-e7cbb08c21ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                   \r"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m----> 5\u001b[0m \u001b[43mseq2seq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[8], line 58\u001b[0m, in \u001b[0;36mSeq2Seq.train\u001b[0;34m(self, train_loader, test_loader, enc_optimizer, dec_optimizer, criterion, device, encoder_scheduler, decoder_scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     55\u001b[0m enc_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     56\u001b[0m dec_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 58\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m preds \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     61\u001b[0m correct \u001b[38;5;241m=\u001b[39m (preds \u001b[38;5;241m==\u001b[39m trg)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "seq2seq.train(\n",
        "    train_loader,\n",
        "    test_loader,\n",
        "    encoder_optimizer,\n",
        "    decoder_optimizer,\n",
        "    criterion,\n",
        "    device,\n",
        "    encoder_scheduler,\n",
        "    decoder_scheduler,\n",
        "    100,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(seq2seq.encoder.state_dict(), \"encoder.pth\")\n",
        "torch.save(seq2seq.decoder.state_dict(), \"decoder.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "for i in range(10):\n",
        "    print(i)\n",
        "    time.sleep(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!shutdown now"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (myenv)",
      "language": "python",
      "name": "torch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
